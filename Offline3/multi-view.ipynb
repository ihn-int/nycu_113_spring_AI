{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Illusion with text-to-image Diffusion\n",
    "Optical illusion is a visual phenomenon that tricks the brain into perceiving something that isn't there or misinterpreting the true nature of an image.\n",
    "\n",
    "This code uses a pretrained diffusion model to generate such images, focusing on different contents from various perspectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Access\n",
    "In this homework, we deploy a `pixel-based` diffusion model named [DeepFloyd IF](https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if). Therefore, it's necessary to obtain the access token from Hugging Face, please follows these steps below:\n",
    "\n",
    "1. Make sure to have a [Hugging Face account](https://huggingface.co/join) and be logged in.\n",
    "2. Accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0).\n",
    "3. Log in locally by entering your [Hugging Face Hub access token](https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens) below, which can be [found here](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "##############################\n",
    "# TODO2-0: Fill your acess token\n",
    "# Begin your code\n",
    "token = \"\"\n",
    "# raise NotImplementedError\n",
    "# End your code\n",
    "##############################\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "Run the cell below to install the required dependencies. You can skip this step if the environment is already setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "! pip install -q   \\\n",
    "    diffusers      \\\n",
    "    transformers   \\\n",
    "    safetensors    \\\n",
    "    sentencepiece  \\\n",
    "    accelerate     \\\n",
    "    bitsandbytes   \\\n",
    "    einops         \\\n",
    "    mediapy        \\\n",
    "    python-time    \\\n",
    "    pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies and Misc Setup\n",
    "We import packages we need and define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import mediapy as mp\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# Convert image ([-1,1] GPU) into image ([0,255] CPU)\n",
    "def im_to_np(im):\n",
    "    im = (im / 2 + 0.5).clamp(0, 1)\n",
    "    im = im.detach().cpu().permute(1, 2, 0).numpy()\n",
    "    im = (im * 255).round().astype(\"uint8\")\n",
    "    return im\n",
    "\n",
    "\n",
    "# Garbage collection function to free memory\n",
    "def flush():\n",
    "    sleep(1)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Set up device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load T5 TextEncoder Model\n",
    "We will load the `T5` text model in half-precision (`fp16`), use it to encode some prompts, and then delete it to recover GPU memory. Note that downloading the model may take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "text_encoder = T5EncoderModel.from_pretrained(\n",
    "    \"DeepFloyd/IF-I-L-v1.0\",\n",
    "    subfolder=\"text_encoder\",\n",
    "    device_map=None,\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-I-L-v1.0\",\n",
    "    text_encoder=text_encoder,  # pass the previously instantiated text encoder\n",
    "    unet=None\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create Text Embeddings\n",
    "\n",
    "We can now use the T5 model to embed prompts for our optical illusion. It may be a good idea to embed a few prompts that you want to use, given that we will delete the T5 text encoder in the next block. See the commented out code for an example of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TODO2-1: Prompt Design\n",
    "# Begin your code\n",
    "prompt_1 = \"A red fox emerging from swirling flames, with fiery orange and gold colors, mystical atmosphere\"\n",
    "prompt_2 = \"A castle tower rising from swirling flames when viewed upside down, with fiery orange and gold colors, mystical atmosphere\"\n",
    "# raise NotImplementedError\n",
    "# End your code\n",
    "##############################\n",
    "\n",
    "# Embed prompts using the T5 model\n",
    "prompts = [prompt_1, prompt_2]\n",
    "prompt_embeds = [pipe.encode_prompt(prompt) for prompt in prompts]\n",
    "prompt_embeds, negative_prompt_embeds = zip(*prompt_embeds)\n",
    "prompt_embeds = torch.cat(prompt_embeds)\n",
    "negative_prompt_embeds = torch.cat(negative_prompt_embeds)  # These are just null embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Transformation\n",
    "To generate multi-view optical illusions, we need to predefine the viewing transformation for the `denoising process`. However, there are some `constraints` on the transformation matrix, including the properties of being `invertible, linear, and orthogonal`. You don't need to worry about these constraints in this homework, but understanding them can be helpful if you want to explore different viewing effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "##############################\n",
    "# TODO2-2: Viewing Transformation\n",
    "# Begin your code\n",
    "class IdentityView:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def view(self, im):\n",
    "        return im\n",
    "    def inverse_view(self, noise):\n",
    "        return noise\n",
    "\n",
    "class Rotate180View:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def view(self, im):\n",
    "        return TF.rotate(im, 180)\n",
    "    def inverse_view(self, noise):\n",
    "        return TF.rotate(noise, 180)\n",
    "\n",
    "views = [IdentityView(), Rotate180View()]\n",
    "\n",
    "# End your code\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Text Encoder\n",
    "\n",
    "We now delete the text encoder (and the `diffusers` pipeline) and flush to free memory for the DeepFloyd image generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del text_encoder\n",
    "del pipe\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Diffusion Process\n",
    "\n",
    "With our now released and available GPU memory, we can load the various DeepFloyd IF diffusion models (also at `float16` precision). This may take a minute of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Load DeepFloyd IF stage I\n",
    "stage_1 = DiffusionPipeline.from_pretrained(\n",
    "                \"DeepFloyd/IF-I-L-v1.0\",\n",
    "                text_encoder=None,\n",
    "                variant=\"fp16\",\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(device)\n",
    "\n",
    "# Load DeepFloyd IF stage II\n",
    "stage_2 = DiffusionPipeline.from_pretrained(\n",
    "                \"DeepFloyd/IF-II-L-v1.0\",\n",
    "                text_encoder=None,\n",
    "                variant=\"fp16\",\n",
    "                torch_dtype=torch.float16,\n",
    "            ).to(device)\n",
    "\n",
    "# Load DeepFloyd IF stage III\n",
    "# (which is just Stable Diffusion 4x Upscaler)\n",
    "stage_3 = DiffusionPipeline.from_pretrained(\n",
    "                \"stabilityai/stable-diffusion-x4-upscaler\",\n",
    "                torch_dtype=torch.float16\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Operation\n",
    "In the vanila DeepFloyd IF, it can directly apply stage_1, stage_2, and stage_3 sequentially to compute an $1024 \\times 1024$ image that related to prompt. Nevertheless, in both stage_1 and stage_2, we need to compute noises from different views and apply them on noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "##############################\n",
    "# TODO2-3: Denoising Operation\n",
    "# The following code only computes and applies the noise\n",
    "# from the first prompt and original view. Please revise\n",
    "# the denoising process according to spec.\n",
    "\n",
    "# Begin your code\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoising_loop(model, noisy_images, prompt_embeds, views, \n",
    "                   timesteps, guidance_scale, generator, noise_level=None, upscaled=None):\n",
    "\n",
    "    num_prompts = prompt_embeds.shape[0]\n",
    "    original_noise_level = noise_level\n",
    "    total_steps = len(timesteps)\n",
    "    \n",
    "    for i, t in enumerate(tqdm(timesteps)):\n",
    "        \n",
    "        progress = i / total_steps\n",
    "        if progress < 0.3:\n",
    "            view_weights = [0.5, 0.5]\n",
    "        elif progress < 0.7:\n",
    "            view_weights = [0.5, 0.5]\n",
    "        else:\n",
    "            view_weights = [0.5, 0.5]\n",
    "        \n",
    "        viewed_images =[]\n",
    "        for view in views:\n",
    "            viewed_images.append(view.view(noisy_images))\n",
    "        viewed_images = torch.cat(viewed_images, dim=0)\n",
    "        \n",
    "        # If upscaled is provided (stage 2), concatenate with noisy_images\n",
    "        if upscaled is not None:\n",
    "            viewed_upscaled = []\n",
    "            for view in views:\n",
    "                viewed_upscaled.append(view.view(upscaled))\n",
    "            viewed_upscaled = torch.cat(viewed_upscaled, dim=0)\n",
    "            model_input = torch.cat([viewed_images, viewed_upscaled], dim=1)\n",
    "        else:\n",
    "            model_input = viewed_images\n",
    "\n",
    "        # Duplicate inputs for CFG\n",
    "        # Model input is: [ neg_0, neg_1, ..., pos_0, pos_1, ... ]\n",
    "        model_input = torch.cat([model_input] * 2)\n",
    "        model_input = model.scheduler.scale_model_input(model_input, t)\n",
    "\n",
    "        current_noise_level = None\n",
    "        if original_noise_level is not None: # stage 2\n",
    "            noise_value = original_noise_level[0] if original_noise_level.numel() > 0 else original_noise_level\n",
    "            current_noise_level = noise_value.unsqueeze(0).repeat(model_input.shape[0])\n",
    "        \n",
    "            \n",
    "        # Predict noise estimate\n",
    "        noise_pred = model.unet(\n",
    "            model_input, t, encoder_hidden_states=prompt_embeds, \n",
    "            class_labels=current_noise_level, cross_attention_kwargs=None, return_dict=False\n",
    "        )[0]\n",
    "\n",
    "        # Extract uncond (neg) and cond noise estimates\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "\n",
    "        # Split into noise estimate and variance estimates\n",
    "        # Split predicted noise and predicted variances\n",
    "        splited_size = model_input.shape[1] // (2 if upscaled is not None else 1)\n",
    "        noise_pred_uncond, _ = noise_pred_uncond.split(splited_size, dim=1)\n",
    "        noise_pred_text, predicted_variance = noise_pred_text.split(splited_size, dim=1)\n",
    "        \n",
    "        # Apply CFG only to noise prediction\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        # Split noise predictions for each view\n",
    "        noise_preds_per_view = noise_pred.chunk(num_prompts, dim=0)\n",
    "        var_preds_per_view = predicted_variance.chunk(num_prompts, dim=0) \n",
    "        \n",
    "        # Apply inverse view transformations to each noise prediction\n",
    "        inverse_viewed_noises = []\n",
    "        inverse_viewed_vars = []\n",
    "        for noise_pred_view, var_pred_view, view in zip(noise_preds_per_view, var_preds_per_view, views):\n",
    "            inverse_viewed_noises.append(view.inverse_view(noise_pred_view))\n",
    "            inverse_viewed_vars.append(view.inverse_view(var_pred_view))\n",
    "\n",
    "        weighted_noise = sum(w * noise for w, noise in zip(view_weights, inverse_viewed_noises))\n",
    "        weighted_var = sum(w * var for w, var in zip(view_weights, inverse_viewed_vars))\n",
    "        \n",
    "        \n",
    "        # Combine averaged noise with averaged predicted variance\n",
    "        combined_prediction = torch.cat([weighted_noise, weighted_var], dim=1)\n",
    "        \n",
    "        # Compute the previous noisy sample x_t -> x_t-1\n",
    "        noisy_images = model.scheduler.step(combined_prediction, t, noisy_images, generator=generator, return_dict=False)[0]\n",
    "\n",
    "    return noisy_images\n",
    "\n",
    "# End your code\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def adjusted_stage_1(model, prompt_embeds, negative_prompt_embeds, views,\n",
    "                   num_inference_steps=100, guidance_scale=7.0, generator=None):\n",
    "\n",
    "    num_prompts = prompt_embeds.shape[0]\n",
    "    assert num_prompts == len(views), \\\n",
    "        \"Number of prompts must match number of views!\"\n",
    "    \n",
    "    height = model.unet.config.sample_size\n",
    "    width = model.unet.config.sample_size\n",
    "\n",
    "    # For CFG\n",
    "    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "    # Setup timesteps\n",
    "    model.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = model.scheduler.timesteps\n",
    "\n",
    "    # Make intermediate_images\n",
    "    noisy_images = model.prepare_intermediate_images(\n",
    "        1, model.unet.config.in_channels, height, width, prompt_embeds.dtype, device, generator,\n",
    "    )\n",
    "\n",
    "    return denoising_loop(model, noisy_images, prompt_embeds, views, \n",
    "                          timesteps, guidance_scale, generator)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def adjusted_stage_2(model, image, prompt_embeds, negative_prompt_embeds, views,\n",
    "                   num_inference_steps=100, guidance_scale=7.0, noise_level=50, generator=None):\n",
    "\n",
    "    num_prompts = prompt_embeds.shape[0]\n",
    "    assert num_prompts == len(views), \\\n",
    "        \"Number of prompts must match number of views!\"\n",
    "        \n",
    "    height = model.unet.config.sample_size\n",
    "    width = model.unet.config.sample_size\n",
    "    num_images_per_prompt = 1\n",
    "\n",
    "    # For CFG\n",
    "    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])\n",
    "\n",
    "    # Get timesteps\n",
    "    model.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = model.scheduler.timesteps\n",
    "\n",
    "    num_channels = model.unet.config.in_channels // 2\n",
    "    noisy_images = model.prepare_intermediate_images(\n",
    "        1, num_channels, height, width, prompt_embeds.dtype, device, generator,\n",
    "    )\n",
    "\n",
    "    # Prepare upscaled image and noise level\n",
    "    image = model.preprocess_image(image, num_images_per_prompt, device)\n",
    "    upscaled = F.interpolate(image, (height, width), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    noise_level = torch.tensor([noise_level] * upscaled.shape[0], device=upscaled.device)\n",
    "    noise = randn_tensor(upscaled.shape, generator=generator, device=upscaled.device, dtype=upscaled.dtype)\n",
    "    upscaled = model.image_noising_scheduler.add_noise(upscaled, noise, timesteps=noise_level)\n",
    "\n",
    "    return denoising_loop(model, noisy_images, prompt_embeds, views, \n",
    "                          timesteps, guidance_scale, generator, noise_level, upscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Illusions\n",
    "Now, we can sample illusions by denoising multiple views at once. The `adjusted_stage_1` function does this and generates a $64 \\times 64$ image. The `adjusted_stage_2` function upsamples the resulting image while denoising all views, and generates a $256 \\times 256$ image.\n",
    "\n",
    "Finally, `stage_3` simply upsamples the $256 \\times 256$ image using a single given text prompt to $1024 \\times 1024$, _without_ doing multi-view denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_64 = adjusted_stage_1(stage_1, prompt_embeds, negative_prompt_embeds, views,\n",
    "                          num_inference_steps=30, guidance_scale=15.0, generator=None)\n",
    "\n",
    "# Show result\n",
    "mp.show_images([im_to_np(view.view(image_64[0])) for view in views])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_256 = adjusted_stage_2(stage_2, image_64, prompt_embeds, negative_prompt_embeds, views,\n",
    "                           num_inference_steps=30, guidance_scale=15.0, noise_level=50, generator=None)\n",
    "\n",
    "# Show result\n",
    "mp.show_images([im_to_np(view.view(image_256[0])) for view in views])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_1024 = stage_3(prompt=prompts[0], image=image_256,\n",
    "                noise_level=0, output_type='pt', generator=None).images\n",
    "image_1024 = image_1024 * 2 - 1\n",
    "\n",
    "# Limit display size, otherwise it's too large for most screens\n",
    "mp.show_images([im_to_np(view.view(image_1024[0])) for view in views], width=400)\n",
    "mp.write_image('result.jpg', im_to_np(image_1024[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Stages and Images\n",
    "We now delete the stages for DeepFloyd image generation and flush to free memory for the CLIP score evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del stage_1\n",
    "del stage_2\n",
    "del stage_3\n",
    "flush()\n",
    "\n",
    "del image_64\n",
    "del image_256\n",
    "del image_1024\n",
    "flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Score\n",
    "This is an evaluation for optical illusion images. A higher score indicates that the text and image are more `closely related`. To ensure the image quality, the score of each image, after applying viewing transformations and comparing it to the corresponding text, `must exceed 0.3`.\n",
    "\n",
    "Note that you can `regenerate` the optical illusion image using the same code until the score is high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP model and processor\n",
    "path = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(path)\n",
    "processor = CLIPProcessor.from_pretrained(path)\n",
    "\n",
    "# Define images and texts\n",
    "image_path = \"result.jpg\"\n",
    "texts = [prompt_1, prompt_2]\n",
    "\n",
    "image = Image.open(image_path)\n",
    "images = [view.view(image) for view in views]\n",
    "\n",
    "# Preprocess\n",
    "inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Use CLIP to compute the embedding\n",
    "outputs = model(**inputs)\n",
    "image_features = outputs.image_embeds\n",
    "text_features = outputs.text_embeds\n",
    "\n",
    "# Calculate the cosine similarities (images <-> texts) with embeddings\n",
    "cosine_similarities = torch.nn.functional.cosine_similarity(image_features, text_features, dim=-1)\n",
    "\n",
    "for text, score in zip(texts, cosine_similarities):\n",
    "    print(f\"Prompt: {text}\")\n",
    "    print(f\"CLIP Score: {score:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
